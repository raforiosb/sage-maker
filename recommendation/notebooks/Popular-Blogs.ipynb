{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from paramiko import RSAKey, Ed25519Key, ECDSAKey, DSSKey, PKey\n",
    "from cryptography.hazmat.primitives import serialization as crypto_serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import ed25519, dsa, rsa, ec\n",
    "\n",
    "def from_private_key( file_obj, password = None ) -> PKey:\n",
    "    private_key = None\n",
    "    file_bytes = bytes( file_obj.read(), \"utf-8\" )\n",
    "    try:\n",
    "        key = crypto_serialization.load_ssh_private_key(\n",
    "            file_bytes,\n",
    "            password = password,\n",
    "        )\n",
    "        file_obj.seek( 0 )\n",
    "    except ValueError:\n",
    "        key = crypto_serialization.load_pem_private_key(\n",
    "            file_bytes,\n",
    "            password = password,\n",
    "        )\n",
    "        if password:\n",
    "            encryption_algorithm = crypto_serialization.BestAvailableEncryption(\n",
    "                password\n",
    "            )\n",
    "        else:\n",
    "            encryption_algorithm = crypto_serialization.NoEncryption()\n",
    "        file_obj = StringIO(\n",
    "            key.private_bytes(\n",
    "                crypto_serialization.Encoding.PEM,\n",
    "                crypto_serialization.PrivateFormat.OpenSSH,\n",
    "                encryption_algorithm,\n",
    "            ).decode( \"utf-8\" )\n",
    "        )\n",
    "    if isinstance( key, rsa.RSAPrivateKey ):\n",
    "        private_key = RSAKey.from_private_key( file_obj, password )\n",
    "    elif isinstance( key, ed25519.Ed25519PrivateKey ):\n",
    "        private_key = Ed25519Key.from_private_key( file_obj, password )\n",
    "    elif isinstance( key, ec.EllipticCurvePrivateKey ):\n",
    "        private_key = ECDSAKey.from_private_key( file_obj, password )\n",
    "    elif isinstance( key, dsa.DSAPrivateKey ):\n",
    "        private_key = DSSKey.from_private_key( file_obj, password )\n",
    "    else:\n",
    "        raise TypeError\n",
    "    return private_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from sqlalchemy import (create_engine, Boolean, Column, ForeignKey,\n",
    "                        Integer, String, DateTime)\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage: prod\n"
     ]
    }
   ],
   "source": [
    "# Dir\n",
    "model_dir = '/'.join(os.getcwd().split(\"/\")[:-1]+[\"model\"])\n",
    "data_dir = '/'.join(os.getcwd().split(\"/\")[:-1]+[\"data\"])\n",
    "CONFIG_DIRECTORY = '/'.join(os.getcwd().split(\"/\")[:-1]+[\"config\"])\n",
    "\n",
    "\n",
    "# Select one staging\n",
    "# STAGE = \"dev\"\n",
    "STAGE = \"prod\"\n",
    "TABLE_NAME = \"wp_posts\"\n",
    "\n",
    "print(\"stage: \"+STAGE)\n",
    "\n",
    "DATABASE_CONFIG = {}\n",
    "\"\"\"Configure database params given the stage\"\"\"\n",
    "\n",
    "if STAGE == 'dev':\n",
    "    DATABASE_CONFIG[\"PEM_FILE\"] = CONFIG_DIRECTORY + \"/\" + \"dataBaseKey.pem\"\n",
    "    DATABASE_CONFIG[\"HOSTNAME\"] = \"koombea20stg.ssh.wpengine.net\"\n",
    "    DATABASE_CONFIG[\"USERNAME\"] = \"koombea20stg\"\n",
    "    DATABASE_CONFIG['PASSWORD'] = 'opypHiPy2GiuCyApXQpZ'\n",
    "    DATABASE_CONFIG[\"SSH_PORT\"] = 22\n",
    "\n",
    "    DATABASE_CONFIG['MYSQL_HOSTNAME'] = '127.0.0.1'\n",
    "    DATABASE_CONFIG['MYSQL_PORT'] = 3306\n",
    "    DATABASE_CONFIG['MYSQL_DBNAME'] = 'wp_koombea20stg'\n",
    "\n",
    "    DATABASE_CONFIG[\"DEV\"] = True\n",
    "\n",
    "    DATABASE_CONFIG[\"PROD_HOSTNAME\"] = \"koombea20.ssh.wpengine.net\"\n",
    "    DATABASE_CONFIG[\"PROD_MYSQL_DBNAME\"] = \"wp_koombea20\"\n",
    "    DATABASE_CONFIG[\"PROD_USERNAME\"] = \"koombea20\"\n",
    "    DATABASE_CONFIG[\"PROD_PASSWORD\"] = \"-WFgRvi2dcg9HDx28JpA\"\n",
    "\n",
    "elif STAGE == \"prod\":\n",
    "    DATABASE_CONFIG[\"PEM_FILE\"] = CONFIG_DIRECTORY + \"/\" + \"dataBaseKey.pem\"\n",
    "    DATABASE_CONFIG[\"HOSTNAME\"] = \"koombea20.ssh.wpengine.net\"\n",
    "    DATABASE_CONFIG[\"USERNAME\"] = \"koombea20\"\n",
    "    DATABASE_CONFIG[\"PASSWORD\"] = \"-WFgRvi2dcg9HDx28JpA\"\n",
    "    DATABASE_CONFIG[\"SSH_PORT\"] = 22\n",
    "\n",
    "    DATABASE_CONFIG['MYSQL_HOSTNAME'] = '127.0.0.1'\n",
    "    DATABASE_CONFIG['MYSQL_PORT'] = 3306\n",
    "    DATABASE_CONFIG[\"MYSQL_DBNAME\"] = \"wp_koombea20\"\n",
    "\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "pkeyfilepath = DATABASE_CONFIG[ 'PEM_FILE' ]\n",
    "pemFile = open(  pkeyfilepath, 'r' )\n",
    "\n",
    "privateKey = from_private_key( pemFile, password = None )\n",
    "\n",
    "tunnel = SSHTunnelForwarder(\n",
    "    ( DATABASE_CONFIG[ 'HOSTNAME' ], DATABASE_CONFIG[ 'SSH_PORT' ] ),\n",
    "    ssh_username = DATABASE_CONFIG[ 'USERNAME' ],\n",
    "    ssh_pkey = privateKey,\n",
    "    remote_bind_address = ( DATABASE_CONFIG[ 'MYSQL_HOSTNAME' ], DATABASE_CONFIG[ 'MYSQL_PORT' ] ), set_keepalive=2.0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnel.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: mysql+pymysql://koombea20:-WFgRvi2dcg9HDx28JpA@127.0.0.1:38523/wp_koombea20\n"
     ]
    }
   ],
   "source": [
    "SQLALCHEMY_DATABASE_URL = \"mysql+pymysql://{}:{}@{}:{}/{}\".format( DATABASE_CONFIG[ 'USERNAME' ],\n",
    "                                                            DATABASE_CONFIG[ 'PASSWORD' ],\n",
    "                                                            DATABASE_CONFIG[ 'MYSQL_HOSTNAME' ],\n",
    "                                                            tunnel.local_bind_port,\n",
    "                                                            DATABASE_CONFIG[ 'MYSQL_DBNAME' ])\n",
    "\n",
    "print(\"URL: \" + SQLALCHEMY_DATABASE_URL)\n",
    "\n",
    "engine = create_engine(\n",
    "    SQLALCHEMY_DATABASE_URL,\n",
    "    pool_pre_ping=True\n",
    ")\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "db = SessionLocal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class Blog(Base):\n",
    "    __tablename__ = \"wp_posts\"\n",
    "    \"\"\"\n",
    "    TABLE wp_posts\n",
    "    Need Columns:\n",
    "        ID: id\n",
    "        post_title: title\n",
    "        post_content: content\n",
    "        post_name: slug\n",
    "    \"\"\"\n",
    "    id = Column('ID', Integer, primary_key=True, index=True)\n",
    "    title = Column('post_title', String)\n",
    "    content = Column('post_content', String)\n",
    "    slug = Column('post_name', String, index=True)\n",
    "    status = Column('post_status', String)\n",
    "    type = Column('post_type', String)\n",
    "    post_date = Column('post_date', DateTime)\n",
    "    post_update = Column('post_modified', DateTime)\n",
    "    post_excerpt = Column('post_excerpt', String)\n",
    "    author_id = Column('post_author', Integer, ForeignKey(\"wp_users.ID\"))\n",
    "    \n",
    "def get_blog_date_by_slug(db, slug:str):\n",
    "    response = db.query(Blog.post_date).filter(Blog.slug == slug).first()\n",
    "    try:\n",
    "        return response[0]\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def sanity_check_slug(db, slugs):\n",
    "    slug_quit = []\n",
    "    for slug in tqdm(slugs, total=len(slugs), desc='sanity check for slugs from trending api'):\n",
    "        date = get_blog_date_by_slug(db, slug)\n",
    "        if date is None:\n",
    "            slug_quit.append(slug)\n",
    "    return slug_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 1, 26, 12, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blog_date_by_slug(db, \"understanding-machine-learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import calendar\n",
    "import datetime\n",
    "import time, os\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data=\"data_export.csv\",\n",
    "    key_folder=\"../config\",\n",
    "    key=\"analytics-key.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/analytics.readonly']\n",
    "KEY_FILE_LOCATION = os.path.join(args.key_folder, args.key)\n",
    "VIEW_ID = '183468851'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize API\n",
    "* Initialize analytics v3\n",
    "* Initialize analyticsreporting v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_analyticsreporting_api():\n",
    "    \"\"\"Initializes an Analytics Reporting API V4 service object.\n",
    "\n",
    "    Returns:\n",
    "        An authorized Analytics Reporting API V4 service object.\n",
    "    \"\"\"\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "            KEY_FILE_LOCATION, SCOPES)\n",
    "\n",
    "    # Build the service object.\n",
    "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
    "\n",
    "    return analytics\n",
    "\n",
    "def initialize_analytics_api():\n",
    "    \"\"\"Initializes a Google Analytics API V3 service object.\n",
    "    \n",
    "    Returns:\n",
    "        An authorized Google Analytics V3 service object\"\"\"\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "            KEY_FILE_LOCATION, SCOPES)\n",
    "\n",
    "    # Build the service object.\n",
    "    analytics = build('analytics', 'v3', credentials=credentials)\n",
    "\n",
    "    return analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_reporting_api = initialize_analyticsreporting_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [{'expression': 'ga:pageviews'},\n",
    "           {'expression': 'ga:uniquePageviews'},\n",
    "           {'expression': 'ga:timeOnPage'},\n",
    "           {'expression': 'ga:avgTimeOnPage'},\n",
    "           #{'expression': 'ga:exits'},\n",
    "           {'expression': 'ga:exitRate'},\n",
    "           {'expression': 'ga:sessions'},\n",
    "           {'expression': 'ga:visits'},\n",
    "           {'expression': 'ga:bounces'},\n",
    "           {'expression': 'ga:bounceRate'},\n",
    "           {'expression': 'ga:sessionDuration'}]\n",
    "\n",
    "dimensions = [{'name': 'ga:pageTitle'},\n",
    "              {'name': 'ga:pagePath'},\n",
    "              {'name': 'ga:pageDepth'}]\n",
    "\n",
    "datetime_now = datetime.datetime.now()\n",
    "# range_year = 4\n",
    "# dates_ranges = [(year, month) for year in range(datetime_now.year - range_year, \n",
    "#                                datetime_now.year + 1) for month in range(1, 12 + 1)]\n",
    "dates_ranges = [(datetime_now.year, month) for month in range(1, datetime_now.month + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2023, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(analytics, date, page_token = None):\n",
    "    \"\"\"Queries the Analytics Reporting API V4.\n",
    "\n",
    "    Args:\n",
    "        analytics: An authorized Analytics Reporting API V4 service object.\n",
    "        date: date ranges body argument request\n",
    "        page_token: page_token just in case\n",
    "    Returns:\n",
    "        The Analytics Reporting API V4 response.\n",
    "    \"\"\"\n",
    "    if page_token is None:\n",
    "        reports = analytics.reports().batchGet(\n",
    "                    body={\n",
    "                        'reportRequests': [\n",
    "                        {\n",
    "                            'viewId': VIEW_ID,\n",
    "                            'dateRanges': date,\n",
    "                            'metrics': metrics,\n",
    "                            'dimensions': dimensions,\n",
    "                            #'samplingLevel': 'LARGE',\n",
    "                            #'pageSize' : 100000\n",
    "                       }]\n",
    "                    }\n",
    "                ).execute()\n",
    "    else:\n",
    "        reports = analytics.reports().batchGet(\n",
    "            body={\n",
    "                        'reportRequests': [\n",
    "                        {\n",
    "                            'viewId': VIEW_ID,\n",
    "                            'dateRanges': date,\n",
    "                            'metrics': metrics,\n",
    "                            'dimensions': dimensions,\n",
    "                            'samplingLevel': 'LARGE',\n",
    "                            'pageToken': page_token,\n",
    "                            'pageSize' : 100000\n",
    "                       }]\n",
    "                    }\n",
    "        ).execute()\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_to_list(report, start_date, end_date):\n",
    "    column_header = report.get('columnHeader', {})\n",
    "    dimension_headers = column_header.get('dimensions', [])\n",
    "    metric_headers = column_header.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
    "    report_list = []\n",
    "    for row in report.get('data', {}).get('rows', []):\n",
    "        data_temp = {}\n",
    "        dimensions = row.get('dimensions', [])\n",
    "        date_range_values = row.get('metrics', [])\n",
    "        \n",
    "        for header, dimension in zip(dimension_headers, dimensions):\n",
    "            data_temp[header] = dimension\n",
    "        for i, values in enumerate(date_range_values):\n",
    "            for metric, value in zip(metric_headers, values.get('values')):\n",
    "                if ',' in value or '.' in value:\n",
    "                    data_temp[metric.get('name')] = float(value)\n",
    "                else:\n",
    "                    data_temp[metric.get('name')] = int(value)\n",
    "        data_temp['startDate'] = start_date\n",
    "        data_temp['endDate'] = end_date\n",
    "        report_list.append(data_temp)\n",
    "    return report_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(analytics, start_date, end_date):\n",
    "    \"\"\" Get data given the start_date and end_date\n",
    "    \n",
    "    Args:\n",
    "        analytics: An authorized Analytics Reporting API V4 service object.\n",
    "        start_date (str): str start_date in the format %Y-%m-%d\n",
    "        end_date (str): str end_date in the format %Y-%m-%d\n",
    "    \"\"\"\n",
    "    report_temp = get_report(analytics, [{'startDate': start_date,\n",
    "                                    'endDate': end_date}])\n",
    "    report = report_temp.get('reports', [])[0]\n",
    "    report_data = report.get('data', {})\n",
    "    if report_data.get('samplesReadCounts', []) or report_data.get('samplingSpaceSizes', []):\n",
    "        return 'Sampled Data'\n",
    "    #if report_data.get('rowCount') > 900000:\n",
    "    #    return 'Exceeded Row Count'\n",
    "    next_page_token = report.get('nextPageToken')\n",
    "    if next_page_token:\n",
    "        print(\"entro\")\n",
    "        raise(\"STOP\")\n",
    "        # Iterating through pages\n",
    "        pass\n",
    "    report = report_to_list(report, start_date, end_date)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_data(analytics, year, month):\n",
    "    \"\"\" Get month data for a given year and month number\n",
    "    \n",
    "    Args:\n",
    "        analytics: An authorized Analytics Reporting API V4 service object.\n",
    "        year (int): year int number\n",
    "        month (int): month int number\n",
    "    \"\"\"\n",
    "    # analytics = initialize_analyticsreporting_api()\n",
    "    last_day_month = calendar.monthrange(year, month)[1] # Get last day of the month\n",
    "    data_list = []\n",
    "    index_day = 1\n",
    "    while index_day < last_day_month:\n",
    "        start_date = \"{:%Y-%m-%d}\".format(datetime.datetime(year, month, index_day))\n",
    "        # index_day += 3\n",
    "        # if (index_day > last_day_month):\n",
    "            # index_day = last_day_month\n",
    "        # end_date = \"{:%Y-%m-%d}\".format(datetime.datetime(year, month, index_day))   \n",
    "        end_date = start_date\n",
    "        while True:\n",
    "#             time.sleep(10)\n",
    "            response = get_data(analytics, start_date, end_date)\n",
    "            if type(response) != str:\n",
    "                data_list += response\n",
    "                break;\n",
    "            else:\n",
    "                index_day -= 1\n",
    "                end_date = \"{:%Y-%m-%d}\".format(datetime.datetime(year, month, index_day))\n",
    "        index_day += 1\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_data(analytics):\n",
    "    \"\"\"Get week data to make a tranding week list of blogs\n",
    "    Args:\n",
    "        analytics: An authorized Analytics Reporting API V4 service object.\n",
    "    \"\"\"\n",
    "    today = datetime.datetime.now()\n",
    "    dates_week = [today + datetime.timedelta(days=i) \n",
    "                  for i in range(0 - today.weekday(), 7 - today.weekday())]\n",
    "    data_list = []\n",
    "    initial_bar = tqdm(dates_week, \n",
    "                   total = len(dates_week), \n",
    "                   desc=\"Getting reports within the actual week\")\n",
    "    for date_week in initial_bar:\n",
    "        start_date = \"{:%Y-%m-%d}\".format(date_week)\n",
    "        end_date = start_date\n",
    "        while True:\n",
    "#             time.sleep(10)\n",
    "            response = get_data(analytics, start_date, end_date)\n",
    "            if type(response) != str:\n",
    "                data_list += response\n",
    "                break;\n",
    "            else:\n",
    "                index_day -= 1\n",
    "                end_date = \"{:%Y-%m-%d}\".format(datetime.datetime(year, month, index_day))\n",
    "        initial_bar.set_postfix(day=date_week.day, lenght=len(data_list))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data within week range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting reports within the actual week: 100%|██████████| 7/7 [00:03<00:00,  1.85it/s, day=29, lenght=3894]\n"
     ]
    }
   ],
   "source": [
    "total_reports_trending = get_week_data(analytics_reporting_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trending = pd.DataFrame(total_reports_trending)\n",
    "data_trending.columns = [col.replace('ga:', '') for col in data_trending.columns]\n",
    "\n",
    "index_blog = data_trending[data_trending['pagePath'].str.contains('blog')].index\n",
    "data_trending = data_trending.loc[index_blog].copy()\n",
    "data_trending = data_trending.loc[~data_trending.pagePath.str.contains(\"__url_version__\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trending['datetime'] = pd.to_datetime(data_trending['startDate'], infer_datetime_format=True)\n",
    "\n",
    "def get_slug(page_path):\n",
    "    page_path_list = page_path.split(\"/\")\n",
    "    try:\n",
    "        if page_path_list[1] == \"blog\":\n",
    "            return page_path_list[2]\n",
    "        else:\n",
    "            return ''\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "data_trending['blog_slug'] = data_trending['pagePath'].apply(get_slug)\n",
    "\n",
    "index_no_blog = data_trending[data_trending['blog_slug'] == ''].index\n",
    "\n",
    "data_trending.drop(index = index_no_blog, inplace=True)\n",
    "data_trending.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity check for slugs from trending api: 100%|██████████| 452/452 [00:38<00:00, 11.65it/s]\n"
     ]
    }
   ],
   "source": [
    "blog_groups = data_trending.groupby('blog_slug').groups\n",
    "\n",
    "# Sanity checks\n",
    "slugs_error = sanity_check_slug(db, blog_groups.keys())\n",
    "data_trending = data_trending[~data_trending['blog_slug'].isin(slugs_error)]\n",
    "blog_groups = data_trending.groupby('blog_slug').groups\n",
    "\n",
    "page_view_sum = {}\n",
    "avg_time_sum = {}\n",
    "post_dates = {}\n",
    "for slug, idx in blog_groups.items():\n",
    "    page_view_sum[slug] = data_trending.loc[idx]['uniquePageviews'].sum()\n",
    "    avg_time_sum[slug] = data_trending.loc[idx]['avgTimeOnPage'].mean()\n",
    "    post_dates[slug] = get_blog_date_by_slug(db, slug)\n",
    "    \n",
    "result = pd.DataFrame([{'slug': slug , 'sum_unique_page_views': page_view_sum[slug],\n",
    "                       'sum_avg_time': np.log(avg_time_sum[slug]+1), 'post_date': post_dates[slug]} \n",
    "                        for slug in page_view_sum.keys()])\n",
    "\n",
    "#result = result.sort_values(by='sum_unique_page_views', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-7-1 2023-1-27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# result.set_index(\"post_date\", inplace=True, drop=True)\n",
    "today = datetime.datetime.now()\n",
    "# today = datetime.datetime(2021, 1, 1)\n",
    "\n",
    "if (today.month - 6)%12 != 0:\n",
    "    start_month = (today.month - 6)%12\n",
    "elif np.abs(today.month - 6) == 6:\n",
    "    start_month = 6\n",
    "else:\n",
    "    start_month = 12\n",
    "    \n",
    "start_date = '{}-{}-{}'.format(today.year if today.month > 6 else today.year - 1, \n",
    "                               start_month, 1)\n",
    "end_date = '{}-{}-{}'.format(today.year, today.month, today.day)\n",
    "print(start_date, end_date)\n",
    "\n",
    "result = result[(result.post_date >= start_date) & (result.post_date <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>sum_unique_page_views</th>\n",
       "      <th>sum_avg_time</th>\n",
       "      <th>post_date</th>\n",
       "      <th>pageRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>web-development-goals</td>\n",
       "      <td>61</td>\n",
       "      <td>6.043388</td>\n",
       "      <td>2023-01-05 10:32:32</td>\n",
       "      <td>20.444283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>software-architect-vs-software-engineer</td>\n",
       "      <td>250</td>\n",
       "      <td>5.165143</td>\n",
       "      <td>2022-08-03 11:58:46</td>\n",
       "      <td>19.471339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>i-have-an-idea-for-an-app-now-what</td>\n",
       "      <td>144</td>\n",
       "      <td>5.297786</td>\n",
       "      <td>2022-09-22 17:00:00</td>\n",
       "      <td>19.280756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>most-profitable-apps</td>\n",
       "      <td>56</td>\n",
       "      <td>5.588185</td>\n",
       "      <td>2023-01-13 11:49:00</td>\n",
       "      <td>19.131150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>how-to-start-a-fintech-company</td>\n",
       "      <td>58</td>\n",
       "      <td>5.533839</td>\n",
       "      <td>2022-07-06 10:09:00</td>\n",
       "      <td>19.018902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        slug  sum_unique_page_views  \\\n",
       "394                    web-development-goals                     61   \n",
       "334  software-architect-vs-software-engineer                    250   \n",
       "206       i-have-an-idea-for-an-app-now-what                    144   \n",
       "263                     most-profitable-apps                     56   \n",
       "198           how-to-start-a-fintech-company                     58   \n",
       "\n",
       "     sum_avg_time           post_date   pageRank  \n",
       "394      6.043388 2023-01-05 10:32:32  20.444283  \n",
       "334      5.165143 2022-08-03 11:58:46  19.471339  \n",
       "206      5.297786 2022-09-22 17:00:00  19.280756  \n",
       "263      5.588185 2023-01-13 11:49:00  19.131150  \n",
       "198      5.533839 2022-07-06 10:09:00  19.018902  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"pageRank\"] = np.log(result[\"sum_unique_page_views\"]+1) + 2.7*result[\"sum_avg_time\"]\n",
    "result = result.sort_values(by='pageRank', ascending=False)\n",
    "result.to_csv(\"../data/trending.csv\", index=False)\n",
    "result.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data within date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting reports within the dates ranges: 100%|██████████| 1/1 [00:10<00:00, 10.05s/it, lenght=19429, month=1, year=2023]\n"
     ]
    }
   ],
   "source": [
    "initial_bar = tqdm(dates_ranges, \n",
    "                   total = len(dates_ranges), \n",
    "                   desc=\"Getting reports within the dates ranges\")\n",
    "\n",
    "total_reports = []\n",
    "for year, month in initial_bar:\n",
    "    report = get_month_data(analytics_reporting_api, year, month)\n",
    "    total_reports += report\n",
    "    initial_bar.set_postfix(year=year, month=month, lenght=len(total_reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(total_reports)\n",
    "data.columns = [col.replace('ga:', '') for col in data.columns]\n",
    "\n",
    "index_blog = data[data['pagePath'].str.contains('blog')].index\n",
    "data = data.loc[index_blog].copy()\n",
    "data = data.loc[~data.pagePath.str.contains(\"__url_version__\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['datetime'] = pd.to_datetime(data['startDate'], infer_datetime_format=True)\n",
    "\n",
    "def get_slug(page_path):\n",
    "    page_path_list = page_path.split(\"/\")\n",
    "    try:\n",
    "        if page_path_list[1] == \"blog\":\n",
    "            return page_path_list[2]\n",
    "        else:\n",
    "            return ''\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "data['blog_slug'] = data['pagePath'].apply(get_slug)\n",
    "\n",
    "index_no_blog = data[data['blog_slug'] == ''].index\n",
    "\n",
    "data.drop(index = index_no_blog, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity check for slugs from trending api:  45%|████▍     | 324/722 [00:27<00:34, 11.64it/s]"
     ]
    }
   ],
   "source": [
    "blogs_ = data.groupby('blog_slug').groups\n",
    "\n",
    "# sanity check\n",
    "slugs_error = sanity_check_slug(db, blogs_.keys())\n",
    "data = data[~data['blog_slug'].isin(slugs_error)]\n",
    "blogs_ = data.groupby('blog_slug').groups\n",
    "\n",
    "avg_time_sum = {}\n",
    "page_view_sum = {}\n",
    "for slug, idx in blogs_.items():\n",
    "    page_view_sum[slug] = data.loc[idx]['uniquePageviews'].sum()\n",
    "    avg_time_sum[slug] = data.loc[idx]['avgTimeOnPage'].mean()\n",
    "\n",
    "result = pd.DataFrame([{'slug': slug , 'sum_unique_page_views': page_view_sum[slug],\n",
    "                       'sum_avg_time': np.log(avg_time_sum[slug]+1)} \n",
    "                        for slug in page_view_sum.keys()])\n",
    "\n",
    "result[\"pageRank\"] = np.log(result[\"sum_unique_page_views\"]+1) + 8*result[\"sum_avg_time\"]\n",
    "result = result.sort_values(by='pageRank', ascending=False)\n",
    "result.to_csv(\"../data/popularity.csv\", index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnel.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38 my_env",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
